import torch
from torch import nn
import numpy as np
import matplotlib.pyplot as plt
import pyro
import pyro.distributions as dist
import torch.nn.functional as F
import torchvision
from pyro.distributions import Normal, Categorical
from pyro.infer import SVI, Trace_ELBO
import torch.optim as optim


device = "cuda" if torch.cuda.is_available() else "cpu"


class NN(nn.Module):
    def __init__(self):
        super(NN, self).__init__()
        self.lin1 = nn.Linear(28,32)
        self.lin2 = nn.Linear(32,32)
        self.lin3 = nn.Linear(32,1)

    def forward(self, x):
        forward = self.lin1(x)
        forward = F.tanh(forward)
        forward = self.lin2(forward)
        forward = F.tanh(forward)
        forward = self.lin3(forward)
        return forward

net = NN().to(device)

def model(x_data, y_data):
    lin1w_prior = dist.Normal(loc=torch.zeros_like(net.lin1.weight),
                         scale=torch.ones_like(net.lin1.weight))
    lin1b_prior = dist.Normal(loc=torch.zeros_like(net.lin1.bias),
                         scale=torch.ones_like(net.lin1.bias))

    lin2w_prior = dist.Normal(loc=torch.zeros_like(net.lin2.weight),
                         scale=torch.ones_like(net.lin2.weight))
    lin2b_prior = dist.Normal(loc=torch.zeros_like(net.lin2.bias),
                         scale=torch.ones_like(net.lin2.bias))

    lin3w_prior = dist.Normal(loc=torch.zeros_like(net.lin3.weight),
                         scale=torch.ones_like(net.lin3.weight))
    lin3b_prior = dist.Normal(loc=torch.zeros_like(net.lin3.bias),
                         scale=torch.ones_like(net.lin3.bias))

    priors = {'lin1.weight': lin1w_prior,
              'lin1.bias': lin1b_prior,
              'lin2.weight': lin2w_prior,
              'lin2.bias:':lin2b_prior,
              'lin3.weight':lin3w_prior,
              'lin3.bias':lin3b_prior}

    lifted_module = pyro.random_module("module",
                                       net,
                                       priors)
    lifted_reg_module = lifted_module()
    kij = lifted_reg_module(x_data)
    pyro.sample('obs',kij , obs=y_data)

def guide(x_data, y_data):
    lin1w_mu = torch.randn_like(net.lin1.weight)
    lin1w_sigma = torch.randn_like(net.lin1.weight)
    lin1w_mu_param = pyro.param("lin1w_mu",lin1w_mu)
    lin1w_sigma_param = nn.Softplus(pyro.param("lin1w_sigma",lin1w_sigma))
    lin1w_prior = Normal(loc=lin1w_sigma_param, scale= lin1w_sigma_param)

    lin1b_mu = torch.randn_like(net.lin1.bias)
    lin1b_sigma = torch.randn_like(net.lin1.bias)
    lin1b_mu_param = pyro.param("lin1b_mu", lin1b_mu)
    lin1b_sigma_param = nn.Softplus(pyro.param("lin1b_sigma", lin1b_sigma))
    lin1b_prior = Normal(loc=lin1b_sigma_param, scale=lin1b_sigma_param)

    lin2w_mu = torch.randn_like(net.lin2.weight)
    lin2w_sigma = torch.randn_like(net.lin2.weight)
    lin2w_mu_param = pyro.param("lin2w_mu",lin2w_mu)
    lin2w_sigma_param = nn.Softplus(pyro.param("lin2w_sigma",lin2w_sigma))
    lin2w_prior = Normal(loc=lin2w_sigma_param, scale= lin2w_sigma_param)

    lin2b_mu = torch.randn_like(net.lin2.bias)
    lin2b_sigma = torch.randn_like(net.lin2.bias)
    lin2b_mu_param = pyro.param("lin2b_mu", lin2b_mu)
    lin2b_sigma_param = nn.Softplus(pyro.param("lin2b_sigma", lin2b_sigma))
    lin2b_prior = Normal(loc=lin2b_sigma_param, scale=lin2b_sigma_param)

    lin3w_mu = torch.randn_like(net.lin3.weight)
    lin3w_sigma = torch.randn_like(net.lin3.weight)
    lin3w_mu_param = pyro.param("lin3w_mu",lin3w_mu)
    lin3w_sigma_param = nn.Softplus(pyro.param("lin3w_sigma",lin3w_sigma))
    lin3w_prior = Normal(loc=lin3w_sigma_param, scale= lin3w_sigma_param)

    lin3b_mu = torch.randn_like(net.lin3.bias)
    lin3b_sigma = torch.randn_like(net.lin3.bias)
    lin3b_mu_param = pyro.param("lin3b_mu", lin1b_mu)
    lin3b_sigma_param = nn.Softplus(pyro.param("lin3b_sigma", lin3b_sigma))
    lin3b_prior = Normal(loc=lin3b_sigma_param, scale=lin3b_sigma_param)

    priors = {'lin1.weight': lin1w_prior,
              'lin1.bias': lin1b_prior,
              'lin2.weight': lin2w_prior,
              'lin2.bias:': lin2b_prior,
              'lin3.weight': lin3w_prior,
              'lin3.bias': lin3b_prior}
    lifted_module = pyro.random_module("module",
                                        net,
                                        priors)
    return lifted_module()



main:

import pickle
from pathlib import Path

import pyro.optim
from torch.utils.data import DataLoader
from Dataset import *
from model_BNN import *
from data_extraction import *


build_dir = Path("build")
if not build_dir.exists():
    build_dir.mkdir()

train_fraction = 0.8
dataset = Dataset_input()

train_size = int(train_fraction * len(dataset))
test_size = len(dataset) - train_size
torch.manual_seed(2)
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])
loader_kwargs = {
    "batch_size": 64,
    "drop_last": False,
    "num_workers": 0,
    "pin_memory": False,
    "persistent_workers": False,
}
train_loader = DataLoader(dataset=train_dataset,
                          **loader_kwargs
                          )
test_loader = DataLoader(dataset=test_dataset,
                         **loader_kwargs
                         )

net = NN().to(device)

optim = pyro.optim.DCTAdam({"lr": 0.01, "betas":(0.90, 0.999)})
svi = SVI(model, guide, optim, loss=Trace_ELBO())

num_iter = 10
loss = 0

for i in range(num_iter):
    loss = 0
    for batch, (X, y) in enumerate(train_loader):
        X = X.to(device)
        y = y.to(device)
        svi.step(X, y)